# Hypr-Claw Performance Analysis

**Date:** 2026-02-24  
**Model Tested:** z-ai/glm5  
**Issue:** Slow response times even with fast models

---

## Executive Summary

The system is experiencing significant performance bottlenecks despite using fast LLM models like GLM-5. The primary issues are:

1. **Excessive context loading** (86KB context file with 2474 lines)
2. **Synchronous blocking operations** in the agent loop
3. **Inefficient progress indicators** spawning async tasks
4. **No compilation optimizations** configured
5. **Excessive cloning** throughout the codebase (197 instances in main.rs alone)
6. **Large message history** without effective compaction

---

## Critical Bottlenecks

### 1. Context File Size (CRITICAL)
**Location:** `/home/bigfoot/hypr-claw/data/context/bigfoot:default.json`
- **Size:** 86KB (2474 lines)
- **Impact:** Loaded on every agent loop iteration
- **Problem:** Contains full conversation history, system state, and metadata

**Evidence:**
```rust
// hypr-claw-app/src/main.rs:127
let mut context = context_manager.load(&session_key).await?;
```

The context is loaded synchronously at startup and contains:
- Full message history
- System state snapshots
- Tool statistics
- Approval history
- Token usage tracking

### 2. Agent Loop Blocking Operations (HIGH)
**Location:** `hypr-claw-runtime/src/agent_loop.rs`

**Problem Areas:**

#### a. Progress Indicator Overhead
```rust
// Line 216-223
let progress_handle = tokio::spawn(async move {
    let mut dots = 0;
    loop {
        tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
        dots = (dots + 1) % 4;
        let dot_str = ".".repeat(dots);
        eprint!("\r\x1B[KðŸ¤” Calling LLM (iteration {}/{}){}   ", iteration + 1, max_iterations, dot_str);
        std::io::Write::flush(&mut std::io::stderr()).ok();
    }
});
```
- Spawns a new async task for every LLM call
- Wakes up every 2 seconds to update dots
- Adds unnecessary overhead

#### b. Session Load/Save on Every Run
```rust
// Line 127
let mut messages = self.session_store.load(session_key).await?;

// Line 168
self.session_store.save(session_key, &messages).await?;
```
- Loads entire session history from disk
- Saves back to disk after every interaction
- No in-memory caching

#### c. Lock Acquisition Overhead
```rust
// Line 104
self.lock_manager.acquire(session_key).await?;
```
- Acquires lock for every message
- Lock timeout: 5 seconds default
- No lock pooling or optimization

### 3. LLM Client Configuration (MEDIUM)
**Location:** `hypr-claw-runtime/src/llm_client.rs`

**Issues:**
```rust
// Line 148
.timeout(Duration::from_secs(60))
```
- 60-second timeout for all requests
- No streaming support visible
- Circuit breaker with 30-second cooldown
- Retry logic with exponential backoff (up to 5 seconds)

**Retry Delays:**
```rust
// Line 277
Duration::from_millis((250_u64.saturating_mul(2_u64.saturating_pow(attempt))).min(5000))
```
- Attempt 0: 250ms
- Attempt 1: 500ms
- Attempt 2: 1000ms
- Attempt 3: 2000ms
- Attempt 4+: 5000ms

### 4. Message Compaction (MEDIUM)
**Location:** `hypr-claw-runtime/src/compactor.rs`

**Current Implementation:**
```rust
// Line 38-48
pub fn compact(&self, messages: Vec<Message>) -> Result<Vec<Message>, RuntimeError> {
    let token_count = self.estimate_tokens(&messages);
    
    if token_count <= self.threshold {
        return Ok(messages);
    }
    
    // Split messages: older half to summarize, newer half to keep
    let split_point = messages.len() / 2;
```

**Problems:**
- Simple character-based token estimation (4 chars = 1 token)
- Only compacts when threshold exceeded
- No proactive compaction
- Threshold not visible in config

### 5. Excessive Cloning (HIGH)
**Location:** Throughout codebase, especially `hypr-claw-app/src/main.rs`

**Statistics:**
- 197 `.clone()` calls in main.rs alone
- Cloning large context objects repeatedly
- No use of references where possible

**Examples:**
```rust
context.session_id = session_key.clone();
context.active_soul_id.clone()
active_soul_id = "safe_assistant".to_string();
```

### 6. No Build Optimizations (MEDIUM)
**Location:** `Cargo.toml`

**Missing:**
- No `[profile.release]` section
- No LTO (Link Time Optimization)
- No codegen-units optimization
- No strip configuration
- Default debug symbols in release builds

---

## Performance Impact Breakdown

### Estimated Latency Per Request

| Component | Estimated Time | Severity |
|-----------|---------------|----------|
| Context loading (86KB) | 50-200ms | HIGH |
| Session store load | 20-100ms | MEDIUM |
| Lock acquisition | 5-50ms | LOW |
| Message compaction check | 10-30ms | LOW |
| Progress indicator spawn | 5-15ms | LOW |
| LLM API call | 500-5000ms | EXTERNAL |
| Session store save | 20-100ms | MEDIUM |
| Context save | 50-200ms | HIGH |
| **Total Overhead** | **160-695ms** | **HIGH** |

**Note:** This overhead is PER MESSAGE in the agent loop. With multiple iterations (tool calls), this compounds significantly.

### Memory Usage

| Component | Size | Impact |
|-----------|------|--------|
| Context file | 86KB | Loaded fully into memory |
| Session history | Unknown | Grows unbounded |
| Tool schemas | ~5-10KB | Sent with every LLM call |
| Message clones | Variable | Multiple copies in memory |

---

## Recommended Fixes (Priority Order)

### 1. CRITICAL: Implement Context Streaming/Pagination
**Impact:** 50-80% reduction in I/O overhead

```rust
// Instead of loading full context:
let mut context = context_manager.load(&session_key).await?;

// Load only essential metadata:
let mut context = context_manager.load_metadata(&session_key).await?;
// Load messages on-demand with pagination
let recent_messages = context_manager.load_recent_messages(&session_key, 50).await?;
```

### 2. CRITICAL: Add In-Memory Session Cache
**Impact:** 70-90% reduction in session I/O

```rust
// Add to agent_loop.rs
use lru::LruCache;

struct SessionCache {
    cache: Arc<Mutex<LruCache<String, Vec<Message>>>>,
}

impl SessionCache {
    fn new(capacity: usize) -> Self {
        Self {
            cache: Arc::new(Mutex::new(LruCache::new(capacity))),
        }
    }
    
    async fn get_or_load(&self, key: &str, store: &SessionStore) -> Result<Vec<Message>> {
        let mut cache = self.cache.lock().unwrap();
        if let Some(messages) = cache.get(key) {
            return Ok(messages.clone());
        }
        drop(cache);
        
        let messages = store.load(key).await?;
        let mut cache = self.cache.lock().unwrap();
        cache.put(key.to_string(), messages.clone());
        Ok(messages)
    }
}
```

### 3. HIGH: Remove Progress Indicator Task Spawning
**Impact:** 10-20ms per LLM call

```rust
// Replace async task with simple counter
let start = Instant::now();
eprint!("ðŸ¤” Calling LLM (iteration {}/{})...", iteration + 1, max_iterations);
std::io::Write::flush(&mut std::io::stderr()).ok();

let response = self.llm_client.call(&reinforced_prompt, messages, tool_schemas).await?;

eprint!("\r\x1B[Kâœ“ LLM responded in {:.1}s\n", start.elapsed().as_secs_f32());
```

### 4. HIGH: Add Cargo Build Optimizations
**Impact:** 15-30% faster execution

```toml
# Add to Cargo.toml
[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
strip = true
panic = "abort"

[profile.release.package."*"]
opt-level = 3
```

### 5. MEDIUM: Reduce Cloning with References
**Impact:** 10-20% memory reduction

```rust
// Instead of:
let active_soul_id = context.active_soul_id.clone();

// Use references where possible:
let active_soul_id = &context.active_soul_id;

// Or use Arc for shared ownership:
let active_soul_id = Arc::new(context.active_soul_id);
```

### 6. MEDIUM: Implement Proactive Message Compaction
**Impact:** Prevents context bloat

```rust
// Add to agent_loop.rs
impl<S, L, D, R, Sum> AgentLoop<S, L, D, R, Sum> {
    async fn run_inner(&self, ...) -> Result<String, RuntimeError> {
        let mut messages = self.session_store.load(session_key).await?;
        
        // Compact BEFORE adding new message
        if messages.len() > 20 {
            messages = self.compactor.compact(messages)?;
        }
        
        messages.push(Message::new(Role::User, json!(user_message)));
        // ...
    }
}
```

### 7. MEDIUM: Add Streaming LLM Support
**Impact:** Perceived latency reduction

```rust
// Add streaming callback to LLMClient
pub async fn call_streaming<F>(
    &self,
    system_prompt: &str,
    messages: &[Message],
    tool_schemas: &[serde_json::Value],
    on_chunk: F,
) -> Result<LLMResponse, RuntimeError>
where
    F: Fn(&str) + Send + Sync,
{
    // Stream response chunks as they arrive
    // Call on_chunk for each token/chunk
}
```

### 8. LOW: Optimize Lock Manager
**Impact:** 5-10ms per request

```rust
// Use parking_lot instead of std::sync
use parking_lot::Mutex;

// Or implement lock-free session isolation
// (sessions are independent, don't need global lock)
```

---

## Quick Wins (Implement First)

### 1. Add Release Profile (5 minutes)
```bash
cat >> Cargo.toml << 'EOF'

[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
strip = true
EOF

cargo build --release
```

### 2. Remove Progress Indicator Spawning (10 minutes)
Replace the `tokio::spawn` progress indicator with a simple print statement.

### 3. Add Message Count Limit (5 minutes)
```rust
// In agent_loop.rs run_inner()
if messages.len() > 50 {
    messages = messages.split_off(messages.len() - 40);
}
```

### 4. Increase Compaction Threshold (2 minutes)
```rust
// Find where Compactor is created and increase threshold
let compactor = Compactor::new(100_000, summarizer); // Increase from default
```

---

## Monitoring Recommendations

### Add Performance Metrics

```rust
// Add to agent_loop.rs
use std::time::Instant;

pub async fn run(&self, ...) -> Result<String, RuntimeError> {
    let total_start = Instant::now();
    
    let lock_start = Instant::now();
    self.lock_manager.acquire(session_key).await?;
    tracing::info!("Lock acquired in {:?}", lock_start.elapsed());
    
    let load_start = Instant::now();
    let mut messages = self.session_store.load(session_key).await?;
    tracing::info!("Session loaded in {:?}", load_start.elapsed());
    
    // ... rest of execution
    
    tracing::info!("Total request time: {:?}", total_start.elapsed());
}
```

### Add Tracing

```bash
# Run with tracing enabled
RUST_LOG=hypr_claw_runtime=debug,hypr_claw_app=debug cargo run --release
```

---

## Expected Performance Improvements

| Optimization | Expected Speedup | Difficulty |
|--------------|------------------|------------|
| Release build optimizations | 15-30% | Easy |
| Remove progress spawning | 10-20ms/call | Easy |
| Session caching | 70-90% I/O | Medium |
| Context pagination | 50-80% I/O | Medium |
| Reduce cloning | 10-20% memory | Medium |
| Message compaction | Prevents bloat | Easy |
| Streaming LLM | Better UX | Hard |

**Combined Impact:** 2-5x faster response times for typical interactions.

---

## Testing Recommendations

### Benchmark Before/After

```rust
// Add to hypr-claw-runtime/benches/agent_loop_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn benchmark_agent_loop(c: &mut Criterion) {
    c.bench_function("agent_loop_single_message", |b| {
        b.iter(|| {
            // Run agent loop with mock components
        });
    });
}

criterion_group!(benches, benchmark_agent_loop);
criterion_main!(benches);
```

### Load Testing

```bash
# Test with multiple concurrent requests
for i in {1..10}; do
    echo "Test message $i" | ./target/release/hypr-claw &
done
wait
```

---

## Conclusion

The system has significant performance bottlenecks that are unrelated to LLM speed. The primary issues are:

1. **I/O overhead** from loading/saving large context files
2. **Lack of caching** for session data
3. **Inefficient progress indicators**
4. **No build optimizations**
5. **Excessive memory cloning**

Implementing the recommended fixes in priority order should result in **2-5x performance improvement** for typical user interactions, with the most critical fixes (session caching and context optimization) providing the largest gains.

The "quick wins" section provides immediate improvements that can be implemented in under 30 minutes total.
